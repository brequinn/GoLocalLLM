# GoLocalLLM
Run Large Language Models and Vision Models locally on iOS with MLX

<p align="center">
  <img src="banner.png" alt="GoLocalLLM Hero Banner" />
</p>

**Run powerful AI models entirely offline on your iPhone.**  
Built with [Apple MLX](https://github.com/ml-explore/mlx). No cloud, no servers, no data collection.

---

## Features
- ðŸ“± Download and run LLMs or VLMs directly on your iPhone  
- ðŸ”’ Fully offline, data never leaves your device  
- âš¡ Optimized for Apple Silicon and MLX framework  
- ðŸ›  Open source and extensible  
- ðŸŒ™ Works in Airplane Mode  

---

## How It Works
1. Select and download a model (LLM or VLM).  
2. Run inference locally using MLX.  
3. Interact through a clean SwiftUI interface.  

---

## Installation
Clone the repository and open the project in Xcode:

```bash
git clone https://github.com/yourusername/GoLocalLLM.git
cd GoLocalLLM
open GoLocalLLM.xcodeproj
