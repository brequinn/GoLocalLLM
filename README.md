# GoLocalLLM
Run Large Language Models and Vision Models locally on iOS with MLX

<p align="center">
 <img width="837" height="375" alt="GoLocalLLM" src="https://github.com/user-attachments/assets/a00f4439-3366-43b4-8afb-d1d7b71ff066" />
</p>

**Run powerful AI models entirely offline on your iPhone.**  
Built with [Apple MLX](https://github.com/ml-explore/mlx). No cloud, no servers, no data collection.

---

## Features
- ðŸ“± Download and run LLMs or VLMs directly on your iPhone  
- ðŸ”’ Fully offline, data never leaves your device  
- âš¡ Optimized for Apple Silicon and MLX framework  
- ðŸ›  Open source and extensible  
- ðŸŒ™ Works in Airplane Mode  

---

## How It Works
1. Select and download a model (LLM or VLM).  
2. Run inference locally using MLX.  
3. Interact through a clean SwiftUI interface.  

---

## Installation
Clone the repository and open the project in Xcode:

```bash
git clone https://github.com/yourusername/GoLocalLLM.git
cd GoLocalLLM
open GoLocalLLM.xcodeproj
